{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing duplicate tuples from the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "# Books Amazon\n",
    "amazon = pandas.read_csv('Data/books_amazon_output.csv')\n",
    "unique_amazon = amazon.drop_duplicates(subset='ISBN-10', keep='first', inplace=False)\n",
    "unique_amazon.to_csv('Data/books_amazon_unique.csv', index = False)\n",
    "\n",
    "# Books Millions\n",
    "millions= pandas.read_csv('Data/books_millions_output.csv')\n",
    "unique_millions = millions.drop_duplicates(subset='ISBN-10', keep='first', inplace=False)\n",
    "unique_millions.to_csv('Data/books_millions_unique.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Add column 'ID'. <br/>\n",
    "-> Ensure fields containing comma must be double quoted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "def clean(inputfile, outputfile):\n",
    "    ID = 'ID'\n",
    "    NAME = 'Name'\n",
    "    CATEGORY = 'Category'\n",
    "    AUTHOR = 'Author'\n",
    "    PRICE = 'Price'\n",
    "    SERIES = 'Series'\n",
    "    PAGES = 'Pages'\n",
    "    PUBLISHER = 'Publisher'\n",
    "    DATE = 'Date'\n",
    "    LANGUAGE = 'Language'\n",
    "    ISBN_10 = 'ISBN_10'\n",
    "    ISBN_13 = 'ISBN_13'\n",
    "    DIMENSIONS = 'Dimensions'\n",
    "    WEIGHT = 'Weight'\n",
    "    COMMA = ','\n",
    "    NEW_LINE = '\\n'\n",
    "\n",
    "\n",
    "\n",
    "    df = pandas.read_csv(inputfile)\n",
    "    df = df.fillna('')\n",
    "    df.insert(0, 'ID', 1)\n",
    "    X = df.as_matrix()\n",
    "    for i in range(0,len(X)):\n",
    "        X[i][0] = i +1\n",
    "        for j in range(0,len(X[0])):\n",
    "            if type(X[i][j]) == str and \"#\" in X[i][j]:\n",
    "                X[i][j] = X[i][j].replace(\"#\",\",\")\n",
    "                X[i][j] = '\"' + X[i][j] + '\"'\n",
    "\n",
    "    header = ID+COMMA+NAME+COMMA+CATEGORY+COMMA+AUTHOR+COMMA+PRICE+COMMA+SERIES+COMMA+PAGES+COMMA+PUBLISHER+COMMA+DATE+COMMA+LANGUAGE+COMMA+ISBN_10+COMMA+ISBN_13+COMMA+DIMENSIONS+COMMA+WEIGHT+NEW_LINE\n",
    "    myFile = open(outputfile, 'w')\n",
    "    myFile.write(header)\n",
    "    with myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        writer.writerows(X)\n",
    "\n",
    "    myFile.close()\n",
    "\n",
    "\n",
    "clean('Data/books_amazon_unique.csv','Data/books_amazon_clean.csv')\n",
    "clean('Data/books_millions_unique.csv','Data/books_millions_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented Attribute Equivalence, Overlap and Rule-bvased blockers. <br/>\n",
    "Selected Rule-based blocker with following rules: <br/>\n",
    "1. Jaccard measure on 'Author' <br/>\n",
    "2. Cosine measure on 'Name' <br/>\n",
    "3. Mel measure on 'Author' \n",
    "<br/><br/>\n",
    "Created candidate tuple pairs in table C, and verified the blocking step using table D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n",
      "0%  100%\n",
      "[##] | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding pairs with missing value...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:19\n"
     ]
    }
   ],
   "source": [
    "import py_entitymatching as em\n",
    "import numpy\n",
    "from numpy import block\n",
    "import pandas as pd\n",
    "\n",
    "#*************** Load data into dataframe*************************\n",
    "A=em.read_csv_metadata('Data/books_amazon_clean.csv',key='ID')\n",
    "B=em.read_csv_metadata('Data/books_millions_clean.csv',key='ID')\n",
    "\n",
    "# Test dataframe\n",
    "#A_test = A.head(10)\n",
    "#B_test = B.head(10)\n",
    "\n",
    "# Blocker 1 AttrEquivalenceBlocker\n",
    "# b1=em.AttrEquivalenceBlocker()\n",
    "# C1=b1.block_tables(A, B, \"Weight\", \"Weight\", l_output_attrs=['Name'], r_output_attrs=['Name'])\n",
    "\n",
    "# Blocker 2 Overlap\n",
    "# b2=em.OverlapBlocker()\n",
    "# C2=b2.block_tables(A,B,'Name','Name',word_level=True,overlap_size=2,l_output_attrs=['Name'], r_output_attrs=['Name'],rem_stop_words= True)\n",
    "\n",
    "# Blocker 3 rule based blocker\n",
    "b4=em.RuleBasedBlocker()\n",
    "#****************** Change feature datatype***************************\n",
    "a_types = em.get_attr_types(A)\n",
    "b_types = em.get_attr_types(B)\n",
    "b_types['Name']= a_types['Name']\n",
    "\n",
    "#******************Create Blocker ***********************************\n",
    "# block_f = em.get_features_for_blocking(A,B,validate_inferred_attr_types=False)\n",
    "block_c = em.get_attr_corres(A,B)\n",
    "block_t = em.get_tokenizers_for_blocking()\n",
    "block_s = em.get_sim_funs_for_blocking()\n",
    "block_f=em.get_features(A,B,a_types,b_types,block_c,block_t,block_s)\n",
    "\n",
    "#******************************** Add Rules *************************\n",
    "b4.add_rule(['Author_Author_jac_dlm_dc0_dlm_dc0(ltuple, rtuple) < 0.2'], block_f)\n",
    "b4.add_rule([' Name_Name_cos_dlm_dc0_dlm_dc0(ltuple, rtuple) < 0.3'], block_f)\n",
    "b4.add_rule(['Author_Author_mel(ltuple, rtuple) < 0.5'], block_f)\n",
    "# Unused Rules\n",
    "# b4.add_rule([' Publisher_Publisher_jac_dlm_dc0_dlm_dc0(ltuple, rtuple) < 0.3'], block_f)\n",
    "# b4.add_rule(['name_name_lev_sim(ltuple, rtuple) < 0.8'],block_f)\n",
    "# b4.add_rule(['Category_Category_lev_sim(ltuple, rtuple) < 0.5'], block_f)\n",
    "# b4.add_rule(['Category_Category_jac_dlm_dc0_dlm_dc0(ltuple, rtuple) < 0.5'], block_f)\n",
    "\n",
    "column_names = ['ID','Name', 'Category','Author','Price','Series','Pages','Publisher','Date','Language','ISBN_10','ISBN_13','Dimensions','Weight']\n",
    "#******************* Blocking step**********************\n",
    "C = b4.block_tables(A, B, l_output_attrs=column_names, r_output_attrs=column_names)\n",
    "C.to_csv('Data/C.csv', index = False)\n",
    "\n",
    "#**************************** Debug Blocking******************************\n",
    "D = em.debug_blocker(C, A, B)\n",
    "D.to_csv('Data/D.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampled 500 tuple pairs from table C for matching. <br/>\n",
    "Labeled these 500 tuple pairs manually (1=true match, else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import py_entitymatching as em\n",
    "\n",
    "A=em.read_csv_metadata('Data/books_amazon_clean.csv',key='ID')\n",
    "B=em.read_csv_metadata('Data/books_millions_clean.csv',key='ID')\n",
    "C=em.read_csv_metadata('Data/C.csv',key='_id', fk_ltable = 'ltable_ID', fk_rtable = 'rtable_ID', ltable = A, rtable = B)\n",
    "\n",
    "S = em.sample_table(C, 500)\n",
    "G = S\n",
    "# Labeled the sample tuple pairs manually\n",
    "G.to_csv('Data/G.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created training set I and test set J  using the labeled tuple pairs.  <br/>\n",
    "Set I and set J contain 250 tuple pairs each. <br/>\n",
    "Implemented 6 learning based matchers using cross-validation.<br/>\n",
    "Dropped ISBN related features. <br/>\n",
    "Based on performance on Training set I, selected Random Forest as the best matcher.<br/>\n",
    "Calculated the performance of all 6 matchers on the test set J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Matcher  Average precision  Average recall  Average f1\n",
      "0  DecisionTree           0.928421        0.924786    0.918968\n",
      "1            RF           0.966013        0.924786    0.941175\n",
      "2           SVM           1.000000        0.199145    0.328333\n",
      "3        LinReg           0.597421        0.838355    0.690269\n",
      "4        LogReg           0.896667        0.882692    0.885463\n",
      "5   Naive Bayes           0.878788        0.969231    0.915246\n",
      "           Name  \\\n",
      "0  DecisionTree   \n",
      "1            RF   \n",
      "2           SVM   \n",
      "3        LinReg   \n",
      "4        LogReg   \n",
      "5   Naive Bayes   \n",
      "\n",
      "                                                                            Matcher  \\\n",
      "0          <py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x7f6fa2523a10>   \n",
      "1          <py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x7f6fa2d616d0>   \n",
      "2        <py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x7f6fa2523cd0>   \n",
      "3  <py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x7f6fa23ec590>   \n",
      "4  <py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x7f6fa1b45e50>   \n",
      "5          <py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x7f6fa1b495d0>   \n",
      "\n",
      "   Num folds  Fold 1    Fold 2    Fold 3    Fold 4    Fold 5  Mean score  \n",
      "0          5  1.0000  0.842105  1.000000  0.800000  1.000000    0.928421  \n",
      "1          5  1.0000  0.941176  1.000000  0.888889  1.000000    0.966013  \n",
      "2          5  1.0000  1.000000  1.000000  1.000000  1.000000    1.000000  \n",
      "3          5  0.6875  0.619048  0.666667  0.388889  0.625000    0.597421  \n",
      "4          5  0.9000  0.888889  1.000000  0.777778  0.916667    0.896667  \n",
      "5          5  1.0000  0.800000  1.000000  0.727273  0.866667    0.878788  \n",
      "           Name  \\\n",
      "0  DecisionTree   \n",
      "1            RF   \n",
      "2           SVM   \n",
      "3        LinReg   \n",
      "4        LogReg   \n",
      "5   Naive Bayes   \n",
      "\n",
      "                                                                            Matcher  \\\n",
      "0          <py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x7f6fa2523a10>   \n",
      "1          <py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x7f6fa2d616d0>   \n",
      "2        <py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x7f6fa2523cd0>   \n",
      "3  <py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x7f6fa23ec590>   \n",
      "4  <py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x7f6fa1b45e50>   \n",
      "5          <py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x7f6fa1b495d0>   \n",
      "\n",
      "   Num folds    Fold 1  Fold 2    Fold 3  Fold 4    Fold 5  Mean score  \n",
      "0          5  1.000000  1.0000  0.777778   1.000  0.846154    0.924786  \n",
      "1          5  0.923077  1.0000  0.777778   1.000  0.923077    0.924786  \n",
      "2          5  0.230769  0.2500  0.111111   0.250  0.153846    0.199145  \n",
      "3          5  0.846154  0.8125  0.888889   0.875  0.769231    0.838355  \n",
      "4          5  0.692308  1.0000  1.000000   0.875  0.846154    0.882692  \n",
      "5          5  0.846154  1.0000  1.000000   1.000  1.000000    0.969231  \n",
      "           Name  \\\n",
      "0  DecisionTree   \n",
      "1            RF   \n",
      "2           SVM   \n",
      "3        LinReg   \n",
      "4        LogReg   \n",
      "5   Naive Bayes   \n",
      "\n",
      "                                                                            Matcher  \\\n",
      "0          <py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x7f6fa2523a10>   \n",
      "1          <py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x7f6fa2d616d0>   \n",
      "2        <py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x7f6fa2523cd0>   \n",
      "3  <py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x7f6fa23ec590>   \n",
      "4  <py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x7f6fa1b45e50>   \n",
      "5          <py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x7f6fa1b495d0>   \n",
      "\n",
      "   Num folds    Fold 1    Fold 2    Fold 3    Fold 4    Fold 5  Mean score  \n",
      "0          5  1.000000  0.914286  0.875000  0.888889  0.916667    0.918968  \n",
      "1          5  0.960000  0.969697  0.875000  0.941176  0.960000    0.941175  \n",
      "2          5  0.375000  0.400000  0.200000  0.400000  0.266667    0.328333  \n",
      "3          5  0.758621  0.702703  0.761905  0.538462  0.689655    0.690269  \n",
      "4          5  0.782609  0.941176  1.000000  0.823529  0.880000    0.885463  \n",
      "5          5  0.916667  0.888889  1.000000  0.842105  0.928571    0.915246  \n",
      "\n",
      "\n",
      " Computing results on set J after training on set I \n",
      " \n",
      "\n",
      "For Decision Tree\n",
      "Precision : 96.36% (53/55)\n",
      "Recall : 98.15% (53/54)\n",
      "F1 : 97.25%\n",
      "False positives : 2 (out of 55 positive predictions)\n",
      "False negatives : 1 (out of 195 negative predictions)\n",
      "**************************************************************\n",
      "For SVM\n",
      "Precision : 88.89% (16/18)\n",
      "Recall : 29.63% (16/54)\n",
      "F1 : 44.44%\n",
      "False positives : 2 (out of 18 positive predictions)\n",
      "False negatives : 38 (out of 232 negative predictions)\n",
      "**************************************************************\n",
      "For Random Forest\n",
      "Precision : 96.0% (48/50)\n",
      "Recall : 88.89% (48/54)\n",
      "F1 : 92.31%\n",
      "False positives : 2 (out of 50 positive predictions)\n",
      "False negatives : 6 (out of 200 negative predictions)\n",
      "**************************************************************\n",
      "For Logistic Regression\n",
      "Precision : 79.69% (51/64)\n",
      "Recall : 94.44% (51/54)\n",
      "F1 : 86.44%\n",
      "False positives : 13 (out of 64 positive predictions)\n",
      "False negatives : 3 (out of 186 negative predictions)\n",
      "**************************************************************\n",
      "For Linear Regression\n",
      "Precision : 57.32% (47/82)\n",
      "Recall : 87.04% (47/54)\n",
      "F1 : 69.12%\n",
      "False positives : 35 (out of 82 positive predictions)\n",
      "False negatives : 7 (out of 168 negative predictions)\n",
      "**************************************************************\n",
      "For Naive Bayes\n",
      "Precision : 80.3% (53/66)\n",
      "Recall : 98.15% (53/54)\n",
      "F1 : 88.33%\n",
      "False positives : 13 (out of 66 positive predictions)\n",
      "False negatives : 1 (out of 184 negative predictions)\n",
      "**************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import py_entitymatching as em\n",
    "\n",
    "seed = 0\n",
    "#******************************** Read Labeled Sample Data ************************************\n",
    "A=em.read_csv_metadata('Data/books_amazon_clean.csv',key='ID')\n",
    "B=em.read_csv_metadata('Data/books_millions_clean.csv',key='ID')\n",
    "G=em.read_csv_metadata('Data/G.csv',key='_id', fk_ltable = 'ltable_ID', fk_rtable = 'rtable_ID', ltable = A, rtable = B)\n",
    "\n",
    "#******************************** Split into Train(I) and Test(J) data*************************\n",
    "IJ = em.split_train_test(G,train_proportion=0.5, random_state=0)\n",
    "I = IJ['train']\n",
    "J = IJ['test']\n",
    "I.to_csv('Data/I.csv', index = False)\n",
    "J.to_csv('Data/J.csv', index = False)\n",
    "\n",
    "#******************************** Instantiating the Learning-Based Matchers*********************\n",
    "\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name = 'Naive Bayes')\n",
    "\n",
    "#******************************** Creating Features *********************************************\n",
    "\n",
    "#************* Change feature datatype*****************\n",
    "a_types = em.get_attr_types(A)\n",
    "b_types = em.get_attr_types(B)\n",
    "b_types['Name']= a_types['Name']\n",
    "\n",
    "match_c = em.get_attr_corres(A,B)\n",
    "match_t = em.get_tokenizers_for_blocking()\n",
    "match_s = em.get_sim_funs_for_blocking()\n",
    "\n",
    "F = em.get_features(A,B,a_types,b_types,match_c, match_t, match_s,)\n",
    "\n",
    "#******************************** Drop Attributes: ISBN_10 and ISBN_13  ************************\n",
    "\n",
    "drop_list_index = [47,48,49,50,51,52]\n",
    "F =F.drop(drop_list_index)\n",
    "\n",
    "#********************************  Extracting Feature Vectors **********************************\n",
    "# Convert the I into a set of feature vectors using F\n",
    "H = em.extract_feature_vecs(I,feature_table= F,attrs_after='label',show_progress=False)\n",
    "\n",
    "\n",
    "#********************************Checking / Impute for missing values **************************\n",
    "# Impute feature vectors with the mean of the column values.\n",
    "H = em.impute_table(H, exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], strategy='mean')\n",
    "\n",
    "\n",
    "#********************************Select the best ML matcher ************************************\n",
    "\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg, nb], table=H, exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'],\n",
    "        k=5,target_attr='label', metric_to_select_matcher='f1', random_state=0)\n",
    "\n",
    "\n",
    "#******************************** Display Results **********************************************\n",
    "print(result['cv_stats'])\n",
    "\n",
    "print(result['drill_down_cv_stats']['precision'])\n",
    "print(result['drill_down_cv_stats']['recall'])\n",
    "print(result['drill_down_cv_stats']['f1'])\n",
    "\n",
    "\n",
    "#******************************** Compute Accuracy of Test Set J********************************\n",
    "\n",
    "#************Function to calculate accuracy ********************\n",
    "def compute_accuracy_J(matcher,return_probs_arg, H, J):\n",
    "    # Train using feature vectors from I\n",
    "    matcher.fit(table=H, exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], target_attr='label')\n",
    "\n",
    "    # Convert J into a set of feature vectors using F\n",
    "    L = em.extract_feature_vecs(J, feature_table=F,\n",
    "                                attrs_after='label', show_progress=False)\n",
    "    # Impute L\n",
    "    L = em.impute_table(L, exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], strategy='mean')\n",
    "\n",
    "    # Predict on L\n",
    "    predictions = matcher.predict(table=L, exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], append=True,\n",
    "                                  target_attr='predicted', inplace=False, return_probs=return_probs_arg, probs_attr='proba')\n",
    "    # print(predictions.head())\n",
    "\n",
    "    # Evaluate the predictions\n",
    "    eval_result = em.eval_matches(predictions, 'label', 'predicted')\n",
    "    em.print_eval_summary(eval_result)\n",
    "\n",
    "\n",
    "\n",
    "#******************************** Compute accuracy for each ML model ********************************\n",
    "\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name = 'Naive Bayes')\n",
    "\n",
    "all_matchers = [dt,svm,rf,lg,ln, nb]\n",
    "return_probs = [True,False,True,True,True, True]\n",
    "ML_model = ['Decision Tree', 'SVM' ,'Random Forest' ,'Logistic Regression' ,'Linear Regression', 'Naive Bayes']\n",
    "print(\"\\n\\n Computing results on set J after training on set I \\n \\n\")\n",
    "for i in range(0,len(all_matchers)):\n",
    "    print(\"For \" + ML_model[i])\n",
    "    compute_accuracy_J(all_matchers[i],return_probs[i],H,J)\n",
    "    print(\"**************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best matcher based on training set I: Random Forest <br/>\n",
    "Performance on training set I: <br/>\n",
    "Precision : 96.6%<br/>\n",
    "Recall : 92.4% <br/>\n",
    "F1 : 94.1% <br/><br/>\n",
    "Performance of Random Forest on test set J:<br/>\n",
    "Precision : 96.0% <br/>\n",
    "Recall : 88.89% <br/>\n",
    "F1 : 92.31%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
